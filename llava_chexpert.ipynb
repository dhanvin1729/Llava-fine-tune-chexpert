{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce4484-1b9f-4fa9-b17c-3701636a9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies of the required versions\n",
    "\n",
    "!pip install accelerate --q\n",
    "!pip install -U \"transformers>=4.39.0\" --q\n",
    "!pip install peft bitsandbytes --q\n",
    "!pip install -U \"trl>=0.8.3\" --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1c1f5-a1d3-4d39-9395-2099f2b28ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model in the required quantization configuration from Hugging Face\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                      quantization_config=quantization_config,\n",
    "                                                      torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6442d-43cb-4309-9a1c-060391b28b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chat template to fine-tune the Llava model\n",
    "\n",
    "LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. \\\n",
    "                        The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n",
    "                        {% for message in messages %}{% if message['role'] == 'user' %}\\\n",
    "                        USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}\\\n",
    "                        {% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de11e78-c6be-4247-b810-24fe4096eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "processor.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd2e36-9f5d-40eb-bff0-1c61b37090d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator function to collate the images and text inputs together in the required format\n",
    "\n",
    "class LLavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            messages = example[\"messages\"]\n",
    "            text = self.processor.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "        #print(\"collator successfully used\")\n",
    "        texts.append(text)\n",
    "        images.append(example[\"images\"][0])\n",
    "\n",
    "        batch = self.processor(texts, images, return_tensors=\"pt\", padding=True)\n",
    "        self.processor.tokenizer.add_tokens([\"<image>\", \"<pad>\"], special_tokens=True) \n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = LLavaDataCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2e8e5-a75e-4974-9f69-cd8b5a2782e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_all_supported_modules(model):\n",
    "    target_modules=[]\n",
    "    supported_modules=(nn.Linear,nn.Embedding,nn.Conv2d,nn.Conv1d)\n",
    "    for name,module in model.named_modules():\n",
    "        if isinstance(module,supported_modules):\n",
    "            target_modules.append(name)\n",
    "    return target_modules\n",
    "\n",
    "target_modules=get_all_supported_modules(model)\n",
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25c2a15-4134-4a2e-abce-88b2ea0bc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/mnt/DATA/dhanvin/logs/\",\n",
    "    #report_to=\"tensorboard\",\n",
    "    learning_rate=1.4e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    #push_to_hub=True,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    bf16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8c33a3-7720-4fb7-8130-dcf376f74bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the Low Rank Adaptation (LoRA)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=target_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60b6e-510e-4e73-89eb-4d944046a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the Supervised Fine-tuning Trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    #train_dataset=train_dataset_1,\n",
    "    #eval_dataset=eval_dataset_1,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",  # need a dummy field\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bcdcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "df_train=pd.read_csv(\"chexpert_chunk_4_extracted/filtered_train.csv\",low_memory=False)\n",
    "df_test=pd.read_csv(\"chexpert_chunk_4_extracted/filtered_test.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac96752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "603fc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "path_to_image = df_train['path_to_image'][0].split('.')[0]+'.png'\n",
    "image_path = os.path.join('chexpert_chunk_4_extracted', path_to_image)\n",
    "\n",
    "samp_dict={'messages': [{'content': [{'index': None,\n",
    "     'text': '\"What does the interval extubation and the new findings suggest?',\n",
    "     'type': 'text'},\n",
    "    {'index': 0, 'text': None, 'type': 'image'}],\n",
    "   'role': 'user'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'The interval extubation and subsequent development of mild-to-moderate pulmonary edema and increasing small bilateral pleural effusions suggest that the removal of the breathing tube might have led to fluid accumulation in the lungs and the space around them.',\n",
    "     'type': 'text'}],\n",
    "   'role': 'assistant'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'What conditions might be indicated by bibasilar opacification?',\n",
    "     'type': 'text'}],\n",
    "   'role': 'user'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'Bibasilar opacification is consistent with either atelectasis, where parts of the lungs collapse or do not inflate properly, or aspiration, where foreign material such as food or liquid enters the lungs.',\n",
    "     'type': 'text'}],\n",
    "   'role': 'assistant'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'What is indicated by the prominent perihilar opacification and reticular prominence?',\n",
    "     'type': 'text'}],\n",
    "   'role': 'user'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'The prominent perihilar opacification and reticular prominence are suggestive of superimposed mild pulmonary edema, indicating fluid accumulation in and around the lung tissue.',\n",
    "     'type': 'text'}],\n",
    "   'role': 'assistant'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': \"Was there any significant change in the patient's condition between the two radiographs?\",\n",
    "     'type': 'text'}],\n",
    "   'role': 'user'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'No, there was no significant interval change between the two radiographs besides a mild interval increase in pulmonary edema.',\n",
    "     'type': 'text'}],\n",
    "   'role': 'assistant'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'What remained unchanged in the radiographic findings?',\n",
    "     'type': 'text'}],\n",
    "   'role': 'user'},\n",
    "  {'content': [{'index': None,\n",
    "     'text': 'The cardiomediastinal silhouette and the presence of a right paratracheal mass remained unchanged in the radiographic findings.\"',\n",
    "     'type': 'text'}],\n",
    "   'role': 'assistant'}],\n",
    " 'images': [Image.open(image_path).convert(\"RGB\").resize((480, 640))]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a962fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "images=[]\n",
    "messages = samp_dict[\"messages\"]\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "messages, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "texts.append(text)\n",
    "images.append(samp_dict[\"images\"][0])\n",
    "\n",
    "batch = processor(texts, images, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a063303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export MY_API_KEY=\"xxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44812305-8b19-471b-a50f-4ed6a2f83567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def generate_vqa_train(index):\n",
    "\n",
    "    report = df_train['report'].iloc[index]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following medical report, generate upto 5 questions and their corresponding answers. \n",
    "    The questions and answers should be reflective of what caused the doctor to give this diagnosis. \n",
    "    Omit irrelevant information like age and other demographics.\n",
    "    Also omit the date and time when the x-rays were taken in your responses.\n",
    "    Focus only on the impressions. \n",
    "    The questions and answers should be seperated just by a single newline character:\n",
    "    \n",
    "    Medical Report:\n",
    "    {report}\n",
    "    \n",
    "    Questions and Answers:\n",
    "    \"\"\"\n",
    "    \n",
    "    url = MY_API_KEY \n",
    "    payload = {'text': prompt}\n",
    "\n",
    "    # response_text = ''\n",
    "    # while not response_text:\n",
    "    #     response = requests.post(url, json=payload)\n",
    "    #     response_text = str(response.text)\n",
    "\n",
    "    response = requests.post(url,json=payload)\n",
    "    response_text=str(response.text)\n",
    "    if(response_text==''):\n",
    "        while(response_text==''):\n",
    "            response = requests.post(url,json=payload)\n",
    "            response_text=str(response.text)\n",
    "\n",
    "\n",
    "    lines=response_text.strip().split('\\\\n\\\\n')\n",
    "    \n",
    "    qa_pairs=[]\n",
    "    for line in lines:\n",
    "        parts=line.split(\"\\\\n\")\n",
    "\n",
    "        if(len(parts)==2):\n",
    "            question,answer=parts\n",
    "            qa_pairs.append(question.strip())\n",
    "            qa_pairs.append(answer.strip())\n",
    "    \n",
    "    conversation = {'messages': [],'images':[]}\n",
    "    \n",
    "    for i in range(0, len(qa_pairs), 2):\n",
    "        question = qa_pairs[i].strip()\n",
    "        answer = qa_pairs[i+1].strip()\n",
    "        \n",
    "        if(i==0):\n",
    "            conversation['messages'].append({\n",
    "            'content': [{'index': None, 'text': question, 'type': 'text'},\n",
    "                        {'index': 0, 'text': None, 'type': 'image'}],\n",
    "            'role': 'user'\n",
    "            })\n",
    "            conversation['messages'].append({\n",
    "                'content': [{'index': None, 'text': answer, 'type': 'text'}],\n",
    "                'role': 'assistant'\n",
    "            })\n",
    "        else:\n",
    "            conversation['messages'].append({\n",
    "            'content': [{'index': None, 'text': question, 'type': 'text'}],\n",
    "            'role': 'user'\n",
    "            })\n",
    "            conversation['messages'].append({\n",
    "                'content': [{'index': None, 'text': answer, 'type': 'text'}],\n",
    "                'role': 'assistant'\n",
    "            })\n",
    "            \n",
    "          \n",
    "    path_to_image = df_train['path_to_image'][index].split('.')[0]+'.png'\n",
    "    image_path = os.path.join('chexpert_chunk_4_extracted', path_to_image)\n",
    "    conversation['images'] = [Image.open(image_path).convert(\"RGB\").resize((480, 640))]\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vqa_train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df589022-6c89-4a38-88c3-4ce19edcff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vqa_test(index):\n",
    "    \n",
    "\n",
    "    report = df_test['report'].iloc[index]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following medical report, generate upto 5 questions and their corresponding answers. \n",
    "    The questions and answers should be reflective of what caused the doctor to give this diagnosis. \n",
    "    Omit irrelevant information like age and other demographics.\n",
    "    Also omit the date and time when the x-rays were taken in your responses.\n",
    "    Focus only on the impressions. \n",
    "    The questions and answers should be seperated just by a single newline character:\n",
    "    \n",
    "    Medical Report:\n",
    "    {report}\n",
    "    \n",
    "    Questions and Answers:\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'http://172.21.23.19:6666/process_text'  \n",
    "    payload = {'text': prompt}\n",
    "    \n",
    "    # response_text = ''\n",
    "    # while not response_text:\n",
    "    #     response = requests.post(url, json=payload)\n",
    "    #     response_text = str(response.text)\n",
    "\n",
    "\n",
    "    response = requests.post(url,json=payload)\n",
    "    response_text=str(response.text)\n",
    "    if(response_text==''):\n",
    "        while(response_text==''):\n",
    "            response = requests.post(url,json=payload)\n",
    "            response_text=str(response.text)\n",
    "    \n",
    "    lines=response_text.strip().split('\\\\n\\\\n')\n",
    "    \n",
    "    qa_pairs=[]\n",
    "    for line in lines:\n",
    "        parts=line.split(\"\\\\n\")\n",
    "\n",
    "        if(len(parts)==2):\n",
    "            question,answer=parts\n",
    "            qa_pairs.append(question.strip())\n",
    "            qa_pairs.append(answer.strip())\n",
    "    \n",
    "    conversation = {'messages': [],'images':[]}\n",
    "    \n",
    "    for i in range(0, len(qa_pairs), 2):\n",
    "        question = qa_pairs[i].strip()\n",
    "        answer = qa_pairs[i+1].strip()\n",
    "        \n",
    "        if(i==0):\n",
    "            conversation['messages'].append({\n",
    "            'content': [{'index': None, 'text': question, 'type': 'text'},\n",
    "                        {'index': 0, 'text': None, 'type': 'image'}],\n",
    "            'role': 'user'\n",
    "            })\n",
    "            conversation['messages'].append({\n",
    "                'content': [{'index': None, 'text': answer, 'type': 'text'}],\n",
    "                'role': 'assistant'\n",
    "            })\n",
    "        else:\n",
    "            conversation['messages'].append({\n",
    "            'content': [{'index': None, 'text': question, 'type': 'text'}],\n",
    "            'role': 'user'\n",
    "            })\n",
    "            conversation['messages'].append({\n",
    "                'content': [{'index': None, 'text': answer, 'type': 'text'}],\n",
    "                'role': 'assistant'\n",
    "            })\n",
    "            \n",
    "          \n",
    "    path_to_image = df_train['path_to_image'][index].split('.')[0]+'.png'\n",
    "    image_path = os.path.join('chexpert_chunk_4_extracted', path_to_image)\n",
    "    conversation['images'] = [Image.open(image_path).convert(\"RGB\").resize((480, 640))]\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_vqa_train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cf7a4db-3ff1-407e-bf45-c54ff68d3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_list=[generate_vqa_train(j) for j in range(32)]\n",
    "#images=[[Image.open(os.path.join(\"chexpert_chunk_4_extracted\",df_train['path_to_image'][j]).split(\".\")[0]+\".png\")] for j in range(32)]\n",
    "#train_dict={'messages': messages,'images': images}\n",
    "#train_list = [{'messages': message, 'images': image} for message, image in zip(messages, images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b443732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_list)):\n",
    "    if test_list[i]['messages']==[]:\n",
    "        print(i)\n",
    "    else:\n",
    "        print(f\"okay{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09a2e54d-ebdf-46e1-9282-072c52c5184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "test_list=[generate_vqa_test(j) for j in range(32)]\n",
    "#images=[[Image.open(os.path.join(\"chexpert_chunk_4_extracted\",df_test['path_to_image'][j]).split(\".\")[0]+\".png\")] for j in range(32)]\n",
    "#eval_dict={'messages': messages,'images': images}\n",
    "#eval_list = [{'messages': message, 'images': image} for message, image in zip(messages, images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ee01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0af97bc-e925-4581-b814-87534396aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_dataset=train_list\n",
    "trainer.eval_dataset=test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bba2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir='/mnt/DATA/dhanvin/llava_chexpert_model'\n",
    "trainer.model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)\n",
    "processor.save_pretrained(model_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480795fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration,AutoProcessor,AutoTokenizer,AutoModelForVisualQuestionAnswering\n",
    "\n",
    "load_directory = \"/mnt/DATA/dhanvin/llava_chexpert_model\"\n",
    "model_ft = LlavaForConditionalGeneration.from_pretrained(load_directory)\n",
    "tokenizer_ft=AutoTokenizer.from_pretrained(load_directory)\n",
    "processor_ft=AutoProcessor.from_pretrained(load_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feacedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b339d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"vqa\",'/mnt/DATA/dhanvin/llava_chexpert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b65245",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(image_path).convert(\"RGB\").resize((480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "image_path=os.path.join('chexpert_chunk_4_extracted',df_test['path_to_image'][3400].split('.')[0]+\".png\")\n",
    "pipe(question= \"USER: \\n\" + \"Describe this Image\" + \"\\nASSISTANT:\"\n",
    ",image = Image.open(image_path).convert(\"RGB\").resize((480, 640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02b7992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom pipeline function\n",
    "def vision_language_pipeline(text, image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\").resize((480, 640))\n",
    "    \n",
    "    # Process the inputs\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Generate outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            pixel_values=inputs['pixel_values']\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predicted_text\n",
    "\n",
    "# Create a pipeline for your custom task\n",
    "def custom_pipeline(text, image_path):\n",
    "    return vision_language_pipeline(text, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = custom_pipeline(text=sample_text, image_path=image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd84aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Load a sample image and text\n",
    "image_path = os.path.join('chexpert_chunk_4_extracted',df_test['path_to_image'][3400].split('.')[0]+\".png\")\n",
    "\n",
    "sample_image = Image.open(image_path).convert(\"RGB\").resize((480, 640))\n",
    "\n",
    "sample_text = \"USER: \\n\" + \"Describe this Image\" + \"\\nASSISTANT:\"\n",
    "\n",
    "# Preprocess the inputs\n",
    "inputs = processor_ft(text=sample_text, images=sample_image, return_tensors=\"pt\", padding=True)\n",
    "inputs.keys()\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model_ft.generate(**inputs)\n",
    "\n",
    "# Decode the generated tokens\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(predicted_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "current",
   "language": "python",
   "name": "current"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
